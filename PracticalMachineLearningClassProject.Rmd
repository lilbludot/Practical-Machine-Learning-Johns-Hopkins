---
title: "Practical Machine Learning Predcition Assignment"
author: "Kinga F."
date: "Sunday, January 25, 2015"
output: word_document
---
##Getting the Data
The data source for this project is from: http://groupware.les.inf.puc-rio.br/har.

Reading in the  training dataset that was downloaded from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv:

```{r}
training <- read.csv("pml-training.csv")
attach(training) #attaching data frame to reduce the length of the variable names associated to it. 
```

Reading is the test set that has been downloaded from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv:

```{r}
testing <- read.csv("pml-testing.csv")

```

##Exploring the Data

```{r}
tail(names(training),24)

```
The response variable is "classe" and the rest of the variables are all potential
predictors of this response variable.   To get an idea of the size of this dataset, here are some basic numbers: 

        - the number of variables is `r ncol(training)`
        
        - the number of observations in this dataset is `r nrow(training)`

In order to have some idea of what the response variable looks like, here is the
summary of it:

```{r}
summary(training$classe)
```

After some further examination of the dataset, there are a few things I need to note:

        1.Some of the values are missing, as in the column "skewness_yaw_belt" and some of the values are "NA", as in the column "max_roll_belt":  
        

```{r}
head(training[,c(16,17,18)])

```

        2. Some of the variables are factor variables with over `r 100` factors:

```{r}
is.factor(kurtosis_roll_belt)
str(kurtosis_roll_belt)

```

##Plotting Predictors
In order to best determine which model to choose to predict "classe", I chose to 
graph some of the predictors in a feature plot.

```{r}
library(ggplot2)
#selecting a few of the more promising predictors to be plotted
col_selection1<- c("roll_belt","pitch_belt", "yaw_belt", "roll_arm", "pitch_arm","yaw_arm", "roll_dumbbell", "pitch_dumbbell","yaw_dumbbell", "roll_forearm", "pitch_forearm", "yaw_forearm")

#creating a feature plot, which I will not run within this .rmd file since it 
#it takes a long time to run, but I will insert the plot into the report separately.  
#featurePlot(x=training[,col_selection1],
#           y = training$classe,
#           plot="pairs")

```

In order to closer examine some of these plots, I plotted many of them separately, here is an example of a close-up:

```{r}
qplot(roll_belt, roll_forearm, colour=classe, data=training)

```

In order to understand what is going with the strange groupings on I created a histogram of "roll_belt" and  of "roll_forearm"

```{r}
par(mfrow=c(1,2))
hist(roll_belt, main = "roll_belt")
hist(roll_forearm, main="roll_forearm")
```

However, the graphs did not help me understand the data any better, other than to note the absence of a normal distribution. There are simply too many variables to dwell on them individually for too long.

##Preprocessing the Data


The first 7 variables in the training data set are:

`r names(training)[1:7]`,

which I removed from the data set since they are not relevant towards predicting "classe." So, 

```{r}
training <- training[,-c(1,2,3,4,5,6,7)]

```

Next, I removed all the columns with missing values from the dataset: 

```{r}
training <-training[,colSums(is.na(training))==0]
```

Then, I found all the columns that are factors, while ignoring the last column which was the response variable "classe."

```{r}
col_names <- c()
n <- ncol(training)-1
for (i in 1:n) {
     if (is.factor(training[,i])){
             col_names <- c(col_names,i)
           }
}
```

I then removed these columns from the data frame, since some of the machine learning algorithms cannot work with factor variables that have over 32 levels.  


```{r}
training <- training[,-col_names]
```

Overall, I have reduced the number of predictive variables from `r 159` to `r ncol(training) - 1 `. 


##Cross Validation Using Random Subsampling and Random Forest


I used a for loop to set up cross validation using random subsampling to fit three 
random forest models to random subsets of the training data, called "trainingSet". I then used these models to predict the "classe" variable of the testing subsets, called "testingSet".   I was hoping for an out of sample error of less than 20%.  

```{r}
library(caret);library(randomForest)
first_seed <- 123355
accuracies <-c()
for (i in 1:3){
       set.seed(first_seed)
       first_seed <- first_seed+1
       trainIndex <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
       trainingSet<- training[trainIndex,]
       testingSet<- training[-trainIndex,]
      # modelFit <- randomForest(classe ~., data = trainingSet)
       #prediction <- predict(modelFit, testingSet)
       #testingSet$rightPred <- prediction == testingSet$classe
       #t<-table(prediction, testingSet$classe)
       #print(t)
       #accuracy <- sum(testingSet$rightPred)/nrow(testingSet)
       #accuracies <- c(accuracies,accuracy)
       #print(accuracy)
}
#accuracies
#mean(accuracies)
```

The mean accuracy of these models turned out to be `r .99`, which is 
a good estimate of the out of sample error.  



##Applying the Random Forest Model to the 20 Test Cases

Here is the model again, recalculated (I did not know how to save the model I created in the for loop) using the original first_seed value.

```{r}

modelFit <- randomForest(classe ~., data = training)
nrow(testing)
prediction <- predict(modelFit, testing)
prediction

```

